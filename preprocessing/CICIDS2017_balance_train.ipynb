{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa61717e",
   "metadata": {},
   "source": [
    "# CICIDS2017   \n",
    "# 单独平衡了训练集\n",
    "------------------------------------------------\n",
    "\n",
    "\n",
    "使用所有的.csv文件建立一个干净的CICIDS2017数据集.\n",
    "\n",
    "\n",
    "</div>\n",
    "    <b>数据集描述:</b>  <a href=\"https://www.unb.ca/cic/datasets/ids-2017.html\">加拿大网络安全研究所（CIC）</a>创建, 由标记的网络流组成。 CICIDS2017包含良性和最新的常见攻击。它由 2,830,743 条记录组成，共有 78 个特征。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79466da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23bc6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", color_codes=True)\n",
    "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e74f6c28",
   "metadata": {},
   "source": [
    "## 清洗数据\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54c8365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\桌面\\苏媛媛-毕设代码\\CICIDS2017\\data\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR  = os.path.join(os.path.abspath(\"../\"), \"data\")\n",
    "IMAGE_DIR = os.path.join(os.path.abspath(\"../\"), \"images\")\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc84d016",
   "metadata": {},
   "source": [
    "### 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621908c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(column):\n",
    "    column = column.strip(' ')\n",
    "    column = column.replace('/', '_')\n",
    "    column = column.replace(' ', '_')\n",
    "    column = column.lower()\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae17d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读所有的.csv文件\n",
    "filenames = glob.glob(os.path.join(DATA_DIR,  'raw', '*.csv'))\n",
    "datasets = [pd.read_csv(filename) for filename in filenames]\n",
    "\n",
    "# 移除空白，重命名列\n",
    "for dataset in datasets:\n",
    "    dataset.columns = [clean_column_name(column) for column in dataset.columns]\n",
    "\n",
    "# 连接数据集\n",
    "dataset = pd.concat(datasets, axis=0, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b55bbcd4",
   "metadata": {},
   "source": [
    "对数据进行初步检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31659aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c77672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)  #前5行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include=[int, float])  #整型，浮点型数据的统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include=[object]).transpose()  # 非数值数据的统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18933890",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55dfbf3e",
   "metadata": {},
   "source": [
    "###  处理重复项"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4379b235",
   "metadata": {},
   "source": [
    "首先在合并八个.csv文件后检查是否有重复项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92696a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.duplicated().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf9a50a3",
   "metadata": {},
   "source": [
    "\n",
    "删除重复项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('删除具有重复值的实例之前的数据大小：', dataset.shape[0], end='\\n\\n')\n",
    "\n",
    "# 删除重复行\n",
    "dataset.drop_duplicates(inplace=True, keep=False, ignore_index=True)\n",
    "\n",
    "print('删除具有重复值的实例之后的数据大小： ', dataset.shape[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29097e80",
   "metadata": {},
   "source": [
    "### 处理缺失值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8d47669",
   "metadata": {},
   "source": [
    "\n",
    "检查每个特征中是否有缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69c5395d",
   "metadata": {},
   "source": [
    "看到有334个缺失值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "693c0c2a",
   "metadata": {},
   "source": [
    "   有以下方法处理缺失值\n",
    "1. 除去相应的例子（行）\n",
    "2. 除去相应的属性（列）\n",
    "3. 缺失值设置为0，平均值，或者中位数\n",
    "4. 插补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum() / dataset.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654bb747",
   "metadata": {},
   "source": [
    " \n",
    "由于数据集足够大，所以可以通过移除实例的方法处理  \n",
    "首先确保缺失值与特定标签无关，然后删除该实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns[dataset.isnull().any()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d76e1b7a",
   "metadata": {},
   "source": [
    "\n",
    "如上，缺失值来源：`flow_bytes_s`流字节率（即每秒传输的数据包字节数）。该特征不影响标签分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3aacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('删除具有缺失值的实例之前的数据大小： ', dataset.shape[0], end='\\n\\n')\n",
    "\n",
    "# 删除缺失行\n",
    "dataset.dropna(axis=0, inplace=True, how=\"any\")\n",
    "\n",
    "print('删除具有缺失的实例之后的数据大小： ', dataset.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b50e3a4b",
   "metadata": {},
   "source": [
    "### 处理无穷值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d03aa00",
   "metadata": {},
   "source": [
    "检查是否所有值都是有限的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isfinite(dataset.drop(['label'], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将无穷大值替换为 NaN \n",
    "dataset.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "\n",
    "# 检查哪些标签与无限值相关 \n",
    "dataset[(dataset['flow_bytes_s'].isnull()) & (dataset['flow_packets_s'].isnull())].label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('删除具有无穷大值的实例之前的数据大小：', dataset.shape[0], end='\\n\\n')\n",
    "\n",
    "# 去除无限值的行\n",
    "dataset.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "print('删除具有无穷大值的实例之后的数据大小：', dataset.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08b3b186",
   "metadata": {},
   "source": [
    "### 处理具有准零标准偏差（quasi null std deviation）的特征"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4650ca6",
   "metadata": {},
   "source": [
    "Standard deviation denoted by sigma (σ) is the average of the squared root differences from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = dataset.std(numeric_only=True)\n",
    "dataset_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找满足阈值的特征\n",
    "constant_features = [column for column, std in dataset_std.iteritems() if std < 0.01]\n",
    "\n",
    "# 丢弃这些常量特征\n",
    "dataset.drop(labels=constant_features, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6035350",
   "metadata": {},
   "source": [
    "例如特征： `bwd_psh_flags`, `fwd_urg_flags`, `bwd_urg_flags`, `cwe_flag_count`, `fwd_avg_bytes_bulk`, `fwd_avg_packets_bulk`, `fwd_avg_bulk_rate`, `bwd_avg_bytes_bulk`, `bwd_avg_packets_bulk`, `bwd_avg_bulk_rate`  并不改变"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e45c0a5",
   "metadata": {},
   "source": [
    "### 观察异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db51ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = dataset.quantile(0.25)\n",
    "Q3 = dataset.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifying outliers with interquartile range使用四分位数间距观察异常值\n",
    "filt = (dataset < (Q1 - 1.5 * IQR)) | (dataset > (Q3 + 1.5 * IQR))\n",
    "print(filt.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=dataset[[\"average_packet_size\", \"avg_bwd_segment_size\"]], orient=\"h\")\n",
    "\n",
    "plt.title('Summary of some variables containing outliers', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c22005f",
   "metadata": {},
   "source": [
    "转换特征类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['flow_bytes_s', 'flow_packets_s']] = dataset[['flow_bytes_s', 'flow_packets_s']].apply(pd.to_numeric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc40097c",
   "metadata": {},
   "source": [
    "## 数据探索\n",
    "------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f81af80",
   "metadata": {},
   "source": [
    "### 相关矩阵（Correlation Matrix）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c19945",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_corr = dataset.corr() #计算列的成对相关性，不包括 NA/null 值。\n",
    "dataset_corr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "sns.set(font_scale=1.0)\n",
    "\n",
    "# cmap = colors.LinearSegmentedColormap.from_list('mycmap', ['#ADD8E6', '#00008B'])\n",
    "ax = sns.heatmap(dataset_corr, annot=False,cmap='Blues')#\n",
    "# fig.savefig(os.path.join(IMAGE_DIR, 'correlation matrix.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13b46d1a",
   "metadata": {},
   "source": [
    "  \n",
    "可以看到，某些特征似乎是高度相关的。因此，可能需要删除它们，因为会带来多余的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立和应用被罩矩阵\n",
    "mask = np.triu(np.ones_like(dataset_corr, dtype=bool)) #返回一个bool类型的相同大小的矩阵，mask为对应的上三角矩阵\n",
    "tri_df = dataset_corr.mask(mask) #应用被罩矩阵\n",
    "\n",
    "# 寻找满足阈值的特征\n",
    "correlated_features = [c for c in tri_df.columns if any(tri_df[c] > 0.98)]\n",
    "\n",
    "# 移除高相关性的元素\n",
    "dataset.drop(labels=correlated_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ec6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "sns.set(font_scale=1.0)\n",
    "ax = sns.heatmap(tri_df, annot=False)\n",
    "# fig.savefig(os.path.join(IMAGE_DIR, 'correlation matrix_dropped.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40ed852d",
   "metadata": {},
   "source": [
    "### 标签分布 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "attack = dataset['label'].value_counts()\n",
    "\n",
    "attack_count = attack.values\n",
    "attack_type = attack.index\n",
    "\n",
    "bar = plt.bar(attack_type, attack_count, align='center')\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, format(height, ','), ha='center', va='bottom')\n",
    "#数据集中不同类型的网络活动的分布\n",
    "plt.title('Distribution of different type of network activity in the dataset')\n",
    "plt.xlabel('Network activity')\n",
    "plt.ylabel('Number of instances')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# fig.savefig(os.path.join(IMAGE_DIR, 'network_activity.pdf'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47e05822",
   "metadata": {},
   "source": [
    " \n",
    "数据集显然是不均衡的  \n",
    "可以合并几个具有相似特征和行为的少数类，形成新的攻击类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7defce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'] = dataset['label'].str.replace('Web Attack �', 'Web Attack', regex=False)\n",
    "\n",
    "# 分组\n",
    "attack_group = {\n",
    "    'BENIGN': 'Benign',\n",
    "    'PortScan': 'PortScan',\n",
    "    'DDoS': 'DDoS',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS', \n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'Heartbleed': 'DoS',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Botnet ARES',\n",
    "    'Web Attack Brute Force': 'Web Attack',\n",
    "    'Web Attack Sql Injection': 'Web Attack',\n",
    "    'Web Attack XSS': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration'\n",
    "}\n",
    "\n",
    "# 建立组标签列\n",
    "dataset['label_category'] = dataset['label'].map(lambda x: attack_group[x])\n",
    "dataset['label_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ddcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "attack = dataset['label_category'].value_counts()\n",
    "\n",
    "attack_count = attack.values\n",
    "attack_type = attack.index\n",
    "\n",
    "bar = plt.bar(attack_type, attack_count, align='center')\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2.0, height, format(height, ','), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.title('Distribution of different type of network activity in the dataset', fontsize=18)\n",
    "plt.xlabel('Network activity', fontsize=16)\n",
    "plt.ylabel('Number of instances', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# fig.savefig(os.path.join(IMAGE_DIR, 'network_activity_category.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = dataset.select_dtypes(exclude=[object]).columns\n",
    "columns = numeric_features.tolist()\n",
    "# X_data = pd.DataFrame(dataset, columns=columns) \n",
    "# y_data = pd.DataFrame(dataset, columns=[\"label_category\"])\n",
    "# # 保存清洗后的数据\n",
    "# X_data.to_pickle(os.path.join(DATA_DIR, 'processed', 'raw/data_features.pkl'))\n",
    "# y_data.to_pickle(os.path.join(DATA_DIR, 'processed', 'raw/data_labels.pkl')) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e67a494",
   "metadata": {},
   "source": [
    "## 划分并平衡数据\n",
    "------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2442fc77",
   "metadata": {},
   "source": [
    "### 划分数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a64248",
   "metadata": {},
   "source": [
    "\n",
    "按照 6：2：2 将清洗后的数据集划分成训练、验证和测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['label_category']\n",
    "features = dataset.drop(labels=['label', 'label_category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdff883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42, stratify=labels)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d728403c",
   "metadata": {},
   "source": [
    "### 缩放数据特征的范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98336b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = features.select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "numeric_features = features.select_dtypes(exclude=[object]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('categoricals', OneHotEncoder(drop='first', sparse=False, handle_unknown='error'), categorical_features),\n",
    "    ('numericals', QuantileTransformer(), numeric_features)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "048c90bb",
   "metadata": {},
   "source": [
    "预处理特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features.tolist()\n",
    "\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train), columns=columns)  #拟合+标准化\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test), columns=columns)  # 标准化\n",
    "X_val = pd.DataFrame(preprocessor.transform(X_val), columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12056cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 顺便处理一下原始数据\n",
    "X_data = pd.DataFrame(preprocessor.fit_transform(dataset), columns=columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4554d9d4",
   "metadata": {},
   "source": [
    "预处理标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb40ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = pd.DataFrame(le.fit_transform(y_train), columns=[\"label\"])\n",
    "y_test = pd.DataFrame(le.transform(y_test), columns=[\"label\"])\n",
    "y_val = pd.DataFrame(le.transform(y_val), columns=[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aa4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.DataFrame(dataset, columns=[\"label_category\"]) #dataset['label_category']\n",
    "y_data = pd.DataFrame(le.transform(y_data), columns=[\"label\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfe9ef07",
   "metadata": {},
   "source": [
    "保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09af5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存清洗后的数据\n",
    "X_data.to_pickle(os.path.join(DATA_DIR, 'processed', 'raw/data_features.pkl'))\n",
    "y_data.to_pickle(os.path.join(DATA_DIR, 'processed', 'raw/data_labels.pkl')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_features.pkl'))\n",
    "# X_val.to_pickle(os.path.join(DATA_DIR, 'processed', 'val/val_features.pkl'))\n",
    "# X_test.to_pickle(os.path.join(DATA_DIR, 'processed', 'test/test_features.pkl'))\n",
    "\n",
    "# y_train.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_labels.pkl'))\n",
    "# y_val.to_pickle(os.path.join(DATA_DIR, 'processed', 'val/val_labels.pkl'))\n",
    "# y_test.to_pickle(os.path.join(DATA_DIR, 'processed', 'test/test_labels.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80ad16b0",
   "metadata": {},
   "source": [
    "   \n",
    "使用`SMOTE`和`RandomUnderSampler`的组合来平衡训练集\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def balance_dataset(X, y, undersampling_strategy, oversampling_strategy):\n",
    "\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=undersampling_strategy, random_state=0)  #抽取数据\n",
    "    X_under, y_under = under_sampler.fit_resample(X, y)\n",
    "    #sampling_strategy包含要对数据集进行采样的信息的字典。键对应于要从中采样的类标签，值是要采样的样本数。random_state随机种子\n",
    "    #X是采样的矩阵，y是每个样本的相应标签\n",
    "\n",
    "    over_sampler = SMOTE(sampling_strategy=oversampling_strategy)\n",
    "    X_bal, y_bal = over_sampler.fit_resample(X_under, y_under)    #对下采样数据进行过采样    ，增加数据\n",
    "    \n",
    "    return X_bal, y_bal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdc7370f",
   "metadata": {},
   "source": [
    "***Label Encoder Transformation***\n",
    "```json\n",
    "{\n",
    "    'Benign': 0,\n",
    "    'DoS': 4,  \n",
    "    'DDoS':3,\n",
    "    'PortScan': 7,\n",
    "    'Brute Force': 2,\n",
    "    'Web Attack': 8,\n",
    "    'Botnet ARES': 1,\n",
    "    'Infiltration':6,\n",
    "    'Heartbleed':5\n",
    "\n",
    "}\n",
    "```\n",
    "```json\n",
    "{\n",
    "    'Benign': 0,\n",
    "    'DoS: 4,\n",
    "    'DDoS': 3,\n",
    "    'PortScan': 6,\n",
    "    'Brute Force': 2,\n",
    "    'Web Attack': 7,\n",
    "    'Botnet ARES': 1,\n",
    "    'Infiltration':5\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Benign          2035505 \n",
    "DoS              192264\n",
    "DDoS             128005\n",
    "PortScan          57305\n",
    "Brute Force        8551\n",
    "Web Attack         2118\n",
    "Botnet ARES        1943\n",
    "Infiltration         36\n",
    "\n",
    "label\n",
    "0        1221303\n",
    "4         115358\n",
    "3          76803\n",
    "6          34383\n",
    "2           5130\n",
    "7           1271\n",
    "1           1166\n",
    "5             22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampling_strategy = {\n",
    "    0: 600000,\n",
    "    4: 115358,\n",
    "    3: 76803,\n",
    "    6: 34383,\n",
    "    2: 5130,\n",
    "    7: 1271,\n",
    "    1: 1166,\n",
    "    5: 22,\n",
    "}\n",
    "\n",
    "oversampling_strategy = {\n",
    "    0: 600000,\n",
    "    4: 115358,\n",
    "    3: 76803,\n",
    "    6: 34383,\n",
    "    2: 25130,\n",
    "    7: 21271,\n",
    "    1: 21166,\n",
    "    5: 522,\n",
    "}\n",
    "\n",
    "# Balance the training set\n",
    "X_train_bal, y_train_bal = balance_dataset(X_train, y_train, undersampling_strategy, oversampling_strategy)\n",
    "\n",
    "# Save the balanced training set\n",
    "X_train_bal.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_features_balanced.pkl'))\n",
    "y_train_bal.to_pickle(os.path.join(DATA_DIR, 'processed', 'train/train_labels_balanced.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "imbalanced = y_train.value_counts()\n",
    "balanced = y_train_bal.value_counts()\n",
    "\n",
    "indexes = np.arange(8)\n",
    "width = 0.4\n",
    "rect1 = plt.bar(indexes, imbalanced.values, width, color=\"steelblue\", label=\"imbalanced\")\n",
    "rect2 = plt.bar(indexes + width, balanced.values, width, color=\"indianred\", label=\"balanced\")\n",
    "\n",
    "def add_text(rect):\n",
    "    \"\"\"Add text to top of each bar.\"\"\"\n",
    "    for r in rect:\n",
    "        h = r.get_height()\n",
    "        plt.text(r.get_x() + r.get_width()/2, h*1.01, s=format(h, \",\") ,fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "add_text(rect1)\n",
    "add_text(rect2)\n",
    "\n",
    "ax.set_xticks(indexes + width / 2)\n",
    "ax.set_xticklabels(['Benign', 'DoS','DDoS', 'PortScan', 'Brute Force', 'Web Attack', 'Botnet ARES','Infiltration'])\n",
    "plt.xlabel('Traffic Activity', fontsize=16)\n",
    "plt.ylabel('Instances', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(IMAGE_DIR, 'balanced_dataset.pdf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2243a99b54b3f7520849058754d86334a0b4300796a3623c48ec2361a889102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
